# -*- coding: utf-8 -*-
"""Copy of Covid Detector model from scratch [95%acc].ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jWD-Fy2dUKfhF09LD6WUQOtGR7XvDljh
"""

from google.colab import files
files.upload()

!pip install -q kaggle

!mkdir ~/.kaggle
!touch ~/.kaggle/kaggle.json

api_token = {"username":"amirkasraamini","key":"e9d3dc0c7d67fe619449aba85338297b"}

import json

with open('/root/.kaggle/kaggle.json', 'w') as file:
    json.dump(api_token, file)

!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d prashant268/chest-xray-covid19-pneumonia #3-class dataset

mkdir destination1

!unzip /content/chest-xray-covid19-pneumonia.zip -d destination1

!unzip /content/nih224folderwise-data.zip -d destination2

import os
import numpy as np
import shutil
import random


make_dir ='/content/destination1/Data'
root_dir = '/content/destination1/Data/train'
classes_dir = ['COVID19','NORMAL','PNEUMONIA']

val_ratio = 0.05

for cls in classes_dir:

  os.makedirs(make_dir +'/val/' + cls)


    # Creating partitions of the data after shuffeling
  src = root_dir + '/'+ cls # Folder to copy images from

  allFileNames = os.listdir(src)
  np.random.shuffle(allFileNames)
  train_FileNames, val_FileNames = np.split(np.array(allFileNames),
                                                              [int(len(allFileNames)* (1 - val_ratio))])


  train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]
  val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]

  print('Total images: ', len(allFileNames))
  print('Training: ', len(train_FileNames))
  print('Validation: ', len(val_FileNames))

  # moving images

  for name in val_FileNames:
    shutil.move(name, make_dir +'/val/' + cls)

import tensorflow as tf
import keras,os
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPool2D , Flatten
from keras.preprocessing.image import ImageDataGenerator
import numpy as np

#train
train_datagen = ImageDataGenerator()
training_set = train_datagen.flow_from_directory('/content/destination1/Data/train',
                                                 target_size = (224, 224), class_mode='categorical', batch_size=32
                                                )

#test
test_datagen = ImageDataGenerator()
test_set = test_datagen.flow_from_directory('/content/destination1/Data/val',
                                                 target_size = (224, 224), class_mode='categorical', batch_size=8
                                                )

#validation
val_datagen = ImageDataGenerator()
val_set = val_datagen.flow_from_directory('/content/destination1/Data/test',
                                            target_size = (224, 224), class_mode='categorical', batch_size=32
                                           )

model = Sequential()

model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding="same", activation="relu"))

model.add(Conv2D(filters=64,kernel_size=(3,3),padding="same", activation="relu"))

model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))


model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))

model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))

model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))


model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))

model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))

model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))


model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))

model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))

model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

model.add(Flatten())
model.add(Dense(units=512,activation="relu"))
model.add(Dense(units=1024,activation="relu"))
model.add(Dense(units=3, activation="softmax"))

from keras.optimizers import Adam
opt = Adam(learning_rate=0.001)

model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])

model.summary()

from keras.callbacks import ModelCheckpoint, EarlyStopping

filepath = '/content/best_Model_weights.h5'
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')
# early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, mode='auto')

results = model.fit(x=training_set, validation_data= val_set, epochs=25,callbacks=checkpoint)

"""# Model accuracy and loss graphs"""

import matplotlib.pyplot as plt
plt.plot(results.history["accuracy"])
plt.plot(results.history['val_accuracy'])
plt.plot(results.history['loss'])
plt.plot(results.history['val_loss'])
plt.title("model accuracy")
plt.ylabel("Accuracy")
plt.xlabel("Epoch")
plt.legend(["Accuracy","Validation Accuracy","loss","Validation Loss"])
plt.show()

"""# Graphical confusion matrix

"""

import matplotlib.pyplot as plt
import itertools

def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):

    """

    This function prints and plots the confusion matrix.

    Normalization can be applied by setting `normalize=True`.

    """

    plt.figure(figsize=(10,10))


    plt.imshow(cm, interpolation='nearest', cmap=cmap)

    plt.title(title)

    plt.colorbar()


    tick_marks = np.arange(len(classes))

    plt.xticks(tick_marks, classes, rotation=45)

    plt.yticks(tick_marks, classes)


    if normalize:

        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

        cm = np.around(cm, decimals=2)

        cm[np.isnan(cm)] = 0.0

        print("Normalized confusion matrix")

    else:

        print('Confusion matrix, without normalization')

    thresh = cm.max() / 2.

    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):

        plt.text(j, i, cm[i, j],

                 horizontalalignment="center",

                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()

    plt.ylabel('True label')

    plt.xlabel('Predicted label')

import numpy as np
from sklearn.metrics import classification_report, confusion_matrix

target_names = []

for key in training_set.class_indices:

    target_names.append(key)


print(target_names)


test_steps_per_epoch = np.math.ceil(test_set.samples / test_set.batch_size)

Y_pred = model.predict(test_set, steps=test_steps_per_epoch)

y_pred = np.argmax(Y_pred, axis=1)

print('Confusion Matrix')

cm = confusion_matrix(test_set.classes, y_pred)

plot_confusion_matrix(cm, target_names, title='Confusion Matrix')

"""# Classification report"""

test_steps_per_epoch = np.math.ceil(test_set.samples / test_set.batch_size)

predictions = model.predict(test_set, steps=test_steps_per_epoch)
# Get most likely class
predicted_classes = np.argmax(predictions, axis=-1)
true_classes = test_set.classes
class_labels = list(test_set.class_indices.keys())

report = classification_report(true_classes, predicted_classes, target_names=class_labels)
print(report)

"""# Saving and Loading the model for prediction"""

from keras.models import model_from_json
from keras.models import load_model

model_json = model.to_json()
with open("/content/best_Model_weights.json", "w") as json_file:
    json_file.write(model_json)

json_file = open('/content/best_Model_weights.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
loaded_model.load_weights('best_Model_weights.h5')

# from keras.models import model_from_json
# from keras.models import load_model

# # serialize model to JSON
# model_json = model.to_json()
# with open("best_Model_weights.json", "w") as json_file:
#     json_file.write(model_json)

# # serialize weights to HDF5
# model.save_weights("best_Model_weights.h5")

# # load json and create model
# json_file = open('best_Model_weights.json', 'r')
# loaded_model_json = json_file.read()
# json_file.close()
# loaded_model = model_from_json(loaded_model_json)

# # load weights into new model
# loaded_model.load_weights("best_Model_weights.h5")
# print("Loaded model from disk")

# loaded_model.save('best_Model_weights.hdf5')
# loaded_model=load_model('best_Model_weights.hdf5')

"""# Deploying the model to Predict a single sample."""

# from keras.preprocessing import image
# img = image.load_img("......",target_size=(224,224))
# img = np.asarray(img)

# img = np.expand_dims(img, axis=0)
# loaded_model.predict(img)

import numpy as np
from keras.preprocessing import image
test_image = image.load_img('......', target_size = (224, 224))
test_image = image.img_to_array(test_image)
test_image = np.expand_dims(test_image, axis = 1)
predicted_classes= np.argmax(loaded_model.predict(test_image), axis=1)

result = loaded_model.predict(test_image)
test_set.class_indices
if result[0][0][0] == 1:
  prediction = 'Normal'
elif result[0][1][0] ==1:
  prediction = 'PNEUMONIA'
else:
  prediction = 'Covid'

print(prediction)
print(result)

"""# the code for the 4-class dataset and its organization."""

!kaggle datasets download -d pratik2901/multiclass-weather-dataset #4-class dataset
mkdir destination2
!unzip /content/multiclass-weather-dataset.zip -d destination2

import os
import numpy as np
import shutil
import random
root_dir = '/content/destination2/Multi-class Weather Dataset/' # data root path
classes_dir = ['Cloudy', 'Rain', 'Shine', 'Sunrise'] #total labels

val_ratio = 0.15
test_ratio = 0.05

for cls in classes_dir:
    os.makedirs(root_dir +'train/' + cls)
    os.makedirs(root_dir +'val/' + cls)
    os.makedirs(root_dir +'test/' + cls)

    # Creating partitions of the data after shuffeling
    src = root_dir + cls # Folder to copy images from

    allFileNames = os.listdir(src)
    np.random.shuffle(allFileNames)
    train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                          [int(len(allFileNames)* (1 - (val_ratio + test_ratio))), 
                                                           int(len(allFileNames)* (1 - test_ratio))])


    train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]
    val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]
    test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]

    print('Total images: ', len(allFileNames))
    print('Training: ', len(train_FileNames))
    print('Validation: ', len(val_FileNames))
    print('Testing: ', len(test_FileNames))

    # Copy-pasting images
    for name in train_FileNames:
      shutil.copy(name, root_dir +'train/' + cls)

    for name in val_FileNames:
      shutil.copy(name, root_dir +'val/' + cls)
      
    for name in test_FileNames:
      shutil.copy(name, root_dir +'test/' + cls)

train_datagen1 = ImageDataGenerator()
training_set1 = train_datagen1.flow_from_directory('/content/destination2/Multi-class Weather Dataset/train',
                                                 target_size = (224, 224), class_mode='categorical', batch_size=8
                                                )

val_datagen1 = ImageDataGenerator()
val_set1 = val_datagen1.flow_from_directory('/content/destination2/Multi-class Weather Dataset/val',
                                            target_size = (224, 224), class_mode='categorical', batch_size=8
                                           )

test_datagen1 = ImageDataGenerator()
test_set1 = test_datagen1.flow_from_directory('/content/destination4/Multi-class Weather Dataset/test',
                                            target_size = (224, 224), class_mode='categorical',batch_size=4
                                           )

loaded_model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])

checkpoint_filepath = 'bff.h5'
model_checkpoint_callback = keras.callbacks.ModelCheckpoint(checkpoint_filepath,save_best_only=True, monitor='val_accuracy')

"""# Training a model trainded on 3-class dataset on a 4-class dataset (still  being prepared and worked on)"""

# results1 = loaded_model.fit(x=training_set1, validation_data= val_set1, epochs=40,callbacks=[model_checkpoint_callback])